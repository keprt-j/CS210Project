{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB Data Acquisition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project: /Users/joshua/datascienceproject\n",
      "✓ Database: /Users/joshua/datascienceproject/data/database/tb_data.db\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "project_root = next((p for p in [Path.cwd()] + list(Path.cwd().parents) \n",
    "                     if (p / 'notebooks' / '01_data_acquisition.ipynb').exists()),\n",
    "                    Path.cwd())\n",
    "os.chdir(project_root)\n",
    "\n",
    "WHO_GHO_API_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_INDICATORS = {\n",
    "    \"tb_incidence\": \"TB_e_inc_tbhiv_num\",\n",
    "    \"tb_mortality\": \"TB_e_mort_exc_tbhiv_num\",\n",
    "}\n",
    "\n",
    "START_YEAR = 2020\n",
    "END_YEAR = 2025\n",
    "YEARS = list(range(START_YEAR, END_YEAR + 1))\n",
    "\n",
    "DATABASE_PATH = str(project_root / \"data\" / \"database\" / \"tb_data.db\")\n",
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"✓ Project: {project_root}\")\n",
    "print(f\"✓ Database: {DATABASE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection(db_path, timeout=30.0):\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path, timeout=timeout)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "    return conn\n",
    "\n",
    "def create_schema(db_path):\n",
    "    conn = get_connection(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS country_metadata (\n",
    "            country_code TEXT PRIMARY KEY,\n",
    "            region_code TEXT,\n",
    "            region_name TEXT,\n",
    "            UNIQUE(country_code)\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS tb_data (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            country_code TEXT NOT NULL,\n",
    "            year INTEGER NOT NULL,\n",
    "            number_of_cases REAL,\n",
    "            number_of_deaths REAL,\n",
    "            mortality_rate REAL,\n",
    "            FOREIGN KEY (country_code) REFERENCES country_metadata(country_code),\n",
    "            UNIQUE(country_code, year)\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_tb_country_year ON tb_data(country_code, year)\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"Database schema created successfully at {db_path}\")\n",
    "\n",
    "def create_unified_table(db_path: str):\n",
    "    conn = get_connection(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(\"DROP VIEW IF EXISTS unified_tb_data\")\n",
    "    except sqlite3.OperationalError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS unified_tb_data\")\n",
    "    except sqlite3.OperationalError:\n",
    "        pass\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE unified_tb_data AS\n",
    "        SELECT \n",
    "            cm.country_code, cm.region_code, cm.region_name,\n",
    "            tb.year, \n",
    "            tb.tb_incidence_num AS number_of_cases,\n",
    "            tb.tb_mortality_num AS number_of_deaths\n",
    "        FROM country_metadata cm\n",
    "        INNER JOIN tb_data tb ON cm.country_code = tb.country_code\n",
    "        ORDER BY cm.country_code, tb.year\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Fetching Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def fetch(indicator: str, years=None):\n",
    "    if years is None:\n",
    "        years = YEARS\n",
    "        \n",
    "    url = f\"{WHO_GHO_API_BASE}/{indicator}\"\n",
    "    response = requests.get(url, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    df = pd.DataFrame(data['value'])\n",
    "    df = df[df['TimeDim'].isin(years)].copy()\n",
    "    \n",
    "    df.rename(columns={\n",
    "        'SpatialDim': 'country_code', 'TimeDim': 'year', 'NumericValue': 'value',\n",
    "        'Low': 'value_low', 'High': 'value_high',\n",
    "        'ParentLocation': 'region_name', 'ParentLocationCode': 'region_code'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df = df[df.get('SpatialDimType', '') == 'COUNTRY'].copy()\n",
    "    df = df.dropna(subset=['country_code', 'year'])\n",
    "    df = df[df['value'] > 0].copy()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Loading Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_old_data(db_path, keep_years=None):\n",
    "    if keep_years is None:\n",
    "        keep_years = YEARS\n",
    "    \n",
    "    conn = get_connection(db_path, timeout=30.0)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    placeholders = ','.join([f':year_{i}' for i in range(len(keep_years))])\n",
    "    params = {f'year_{i}': year for i, year in enumerate(keep_years)}\n",
    "    \n",
    "    cursor.execute(f\"\"\"\n",
    "        DELETE FROM tb_data \n",
    "        WHERE year NOT IN ({placeholders})\n",
    "    \"\"\", params)\n",
    "    deleted = cursor.rowcount\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    if deleted > 0:\n",
    "        print(f\"Cleaned up {deleted} records outside year range {min(keep_years)}-{max(keep_years)}\")\n",
    "\n",
    "def load_country_metadata(df, db_path):\n",
    "    conn = get_connection(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    country_df = df[['country_code', 'region_code', 'region_name']].drop_duplicates()\n",
    "    country_df = country_df.dropna(subset=['country_code'])\n",
    "    \n",
    "    for _, row in country_df.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO country_metadata (country_code, region_code, region_name)\n",
    "            VALUES (:country_code, :region_code, :region_name)\n",
    "        \"\"\", {\n",
    "            'country_code': row['country_code'],\n",
    "            'region_code': row.get('region_code'),\n",
    "            'region_name': row.get('region_name')\n",
    "        })\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def load_tb_incidence(df, db_path):\n",
    "    df = df[df['year'].isin(YEARS)].copy()\n",
    "    load_country_metadata(df, db_path)\n",
    "    \n",
    "    conn = get_connection(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    inserted = 0\n",
    "    for _, row in df.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tb_data \n",
    "            (country_code, year, tb_incidence_num)\n",
    "            VALUES (:country_code, :year, :value)\n",
    "            ON CONFLICT(country_code, year) DO UPDATE SET\n",
    "                tb_incidence_num = excluded.tb_incidence_num\n",
    "        \"\"\", {\n",
    "            'country_code': row['country_code'],\n",
    "            'year': int(row['year']),\n",
    "            'value': row.get('value')\n",
    "        })\n",
    "        inserted += 1\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"Loaded {inserted} TB incidence records (years {min(df['year'])}-{max(df['year'])})\")\n",
    "\n",
    "def load_tb_mortality(df, db_path):\n",
    "    df = df[df['year'].isin(YEARS)].copy()\n",
    "    load_country_metadata(df, db_path)\n",
    "    \n",
    "    conn = get_connection(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    inserted = 0\n",
    "    for _, row in df.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO tb_data \n",
    "            (country_code, year, tb_mortality_num)\n",
    "            VALUES (:country_code, :year, :value)\n",
    "            ON CONFLICT(country_code, year) DO UPDATE SET\n",
    "                tb_mortality_num = excluded.tb_mortality_num\n",
    "        \"\"\", {\n",
    "            'country_code': row['country_code'],\n",
    "            'year': int(row['year']),\n",
    "            'value': row.get('value')\n",
    "        })\n",
    "        inserted += 1\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"Loaded {inserted} TB mortality records (years {min(df['year'])}-{max(df['year'])})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition Orchestration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(df, filename):\n",
    "    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    filepath = RAW_DATA_DIR / filename\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Saved {len(df)} records to {filepath}\")\n",
    "\n",
    "def acquire_all():\n",
    "    datasets = {}\n",
    "    \n",
    "    for key, indicator in [('tb_incidence', WHO_INDICATORS['tb_incidence']),\n",
    "                          ('tb_mortality', WHO_INDICATORS['tb_mortality'])]:\n",
    "        df = fetch(indicator, YEARS)\n",
    "        datasets[key] = df\n",
    "        save_csv(df, f'{key}.csv')\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def load_all(datasets, db_path, cleanup=True):\n",
    "    if cleanup:\n",
    "        cleanup_old_data(db_path, YEARS)\n",
    "    \n",
    "    loaders = {\n",
    "        'tb_incidence': load_tb_incidence,\n",
    "        'tb_mortality': load_tb_mortality\n",
    "    }\n",
    "    \n",
    "    for key, loader in loaders.items():\n",
    "        loader(datasets[key], db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Create Database Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database schema created successfully at /Users/joshua/datascienceproject/data/database/tb_data.db\n"
     ]
    }
   ],
   "source": [
    "create_schema(DATABASE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Fetch Data from APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 835 records to /Users/joshua/datascienceproject/data/raw/tb_incidence.csv\n",
      "Saved 884 records to /Users/joshua/datascienceproject/data/raw/tb_mortality.csv\n",
      "Fetched 2 datasets\n"
     ]
    }
   ],
   "source": [
    "datasets = acquire_all()\n",
    "print(f\"Fetched {len(datasets)} datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Load Data into Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 835 TB incidence records (years 2020-2024)\n",
      "Loaded 884 TB mortality records (years 2020-2024)\n"
     ]
    }
   ],
   "source": [
    "load_all(datasets, DATABASE_PATH)\n",
    "create_unified_table(DATABASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Verify Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country_code region_code            region_name  year  number_of_cases  \\\n",
      "0            AFG         EMR  Eastern Mediterranean  2020             13.0   \n",
      "1            AGO         AFR                 Africa  2020          15000.0   \n",
      "2            ALB         EUR                 Europe  2020              4.0   \n",
      "3            ARE         EMR  Eastern Mediterranean  2020              5.0   \n",
      "4            ARG         AMR               Americas  2020            730.0   \n",
      "..           ...         ...                    ...   ...              ...   \n",
      "880          VUT         WPR        Western Pacific  2024              2.0   \n",
      "881          YEM         EMR  Eastern Mediterranean  2024            120.0   \n",
      "882          ZAF         AFR                 Africa  2024         134000.0   \n",
      "883          ZMB         AFR                 Africa  2024          19000.0   \n",
      "884          ZWE         AFR                 Africa  2024          20000.0   \n",
      "\n",
      "     number_of_deaths  \n",
      "0             14000.0  \n",
      "1             21000.0  \n",
      "2                 9.0  \n",
      "3                64.0  \n",
      "4               580.0  \n",
      "..                ...  \n",
      "880              27.0  \n",
      "881            2100.0  \n",
      "882           25000.0  \n",
      "883            4100.0  \n",
      "884            3000.0  \n",
      "\n",
      "[885 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conn = get_connection(DATABASE_PATH)\n",
    "summary = pd.read_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM unified_tb_data\n",
    "    ORDER BY year\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
