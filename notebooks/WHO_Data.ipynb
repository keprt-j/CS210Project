{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB Data Acquisition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project: /Users/joshua/datascienceproject/notebooks\n",
      "✓ Database: /Users/joshua/datascienceproject/notebooks/data/database/tb_data.db\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "project_root = next((p for p in [Path.cwd()] + list(Path.cwd().parents) \n",
    "                     if (p / 'notebooks' / '01_data_acquisition.ipynb').exists()),\n",
    "                    Path.cwd())\n",
    "os.chdir(project_root)\n",
    "\n",
    "WHO_GHO_API_BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "WHO_INDICATORS = {\n",
    "    \"tb_incidence\": \"TB_e_inc_tbhiv_num\",\n",
    "    \"tb_mortality\": \"TB_e_mort_exc_tbhiv_num\",\n",
    "}\n",
    "\n",
    "START_YEAR = 2020\n",
    "END_YEAR = 2025\n",
    "YEARS = list(range(START_YEAR, END_YEAR + 1))\n",
    "\n",
    "DATABASE_PATH = str(project_root / \"data\" / \"database\" / \"tb_data.db\")\n",
    "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"✓ Project: {project_root}\")\n",
    "print(f\"✓ Database: {DATABASE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Fetching Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Create Database Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database schema created successfully at /Users/joshua/datascienceproject/notebooks/data/database/tb_data.db\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.dirname(DATABASE_PATH), exist_ok=True)\n",
    "\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        conn = sqlite3.connect(DATABASE_PATH, timeout=30.0)\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "        cursor = conn.cursor()\n",
    "        break\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if \"locked\" in str(e).lower() and attempt < 4:\n",
    "            time.sleep(1.0 * (attempt + 1))\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS country_metadata (\n",
    "        country_code TEXT PRIMARY KEY,\n",
    "        region_code TEXT,\n",
    "        region_name TEXT,\n",
    "        UNIQUE(country_code)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS tb_data\")\n",
    "        conn.commit()\n",
    "        break\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if \"locked\" in str(e).lower() and attempt < 4:\n",
    "            conn.rollback()\n",
    "            time.sleep(1.0 * (attempt + 1))\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE tb_data (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        country_code TEXT NOT NULL,\n",
    "        year INTEGER NOT NULL,\n",
    "        tb_incidence_num REAL,\n",
    "        tb_mortality_num REAL,\n",
    "        FOREIGN KEY (country_code) REFERENCES country_metadata(country_code),\n",
    "        UNIQUE(country_code, year)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_tb_country_year ON tb_data(country_code, year)\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "time.sleep(0.5)\n",
    "print(f\"Database schema created successfully at {DATABASE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 835 records to /Users/joshua/datascienceproject/notebooks/data/raw/tb_incidence.csv\n",
      "Saved 884 records to /Users/joshua/datascienceproject/notebooks/data/raw/tb_mortality.csv\n",
      "Fetched 2 datasets\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "for key, indicator in [('tb_incidence', WHO_INDICATORS['tb_incidence']),\n",
    "                      ('tb_mortality', WHO_INDICATORS['tb_mortality'])]:\n",
    "    url = f\"{WHO_GHO_API_BASE}/{indicator}\"\n",
    "    response = requests.get(url, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    df = pd.DataFrame(data['value'])\n",
    "    df = df[df['TimeDim'].isin(YEARS)].copy()\n",
    "    \n",
    "    df.rename(columns={\n",
    "        'SpatialDim': 'country_code', 'TimeDim': 'year', 'NumericValue': 'value',\n",
    "        'Low': 'value_low', 'High': 'value_high',\n",
    "        'ParentLocation': 'region_name', 'ParentLocationCode': 'region_code'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df = df[df.get('SpatialDimType', '') == 'COUNTRY'].copy()\n",
    "    df = df.dropna(subset=['country_code', 'year'])\n",
    "    df = df[df['value'] > 0].copy()\n",
    "    \n",
    "    datasets[key] = df\n",
    "    \n",
    "    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    filepath = RAW_DATA_DIR / f'{key}.csv'\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Saved {len(df)} records to {filepath}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Fetched {len(datasets)} datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Load Data into Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 835 TB tb_incidence records (years 2020-2024)\n",
      "Loaded 884 TB tb_mortality records (years 2020-2024)\n",
      "Data acquisition complete\n"
     ]
    }
   ],
   "source": [
    "keep_years = YEARS\n",
    "\n",
    "time.sleep(0.5)\n",
    "\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        conn = sqlite3.connect(DATABASE_PATH, timeout=30.0)\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "        cursor = conn.cursor()\n",
    "        break\n",
    "    except sqlite3.OperationalError as e:\n",
    "        if \"locked\" in str(e).lower() and attempt < 4:\n",
    "            time.sleep(1.0 * (attempt + 1))\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "placeholders = ','.join([f':year_{i}' for i in range(len(keep_years))])\n",
    "params = {f'year_{i}': year for i, year in enumerate(keep_years)}\n",
    "\n",
    "cursor.execute(f\"\"\"\n",
    "    DELETE FROM tb_data \n",
    "    WHERE year NOT IN ({placeholders})\n",
    "\"\"\", params)\n",
    "deleted = cursor.rowcount\n",
    "\n",
    "if deleted > 0:\n",
    "    print(f\"Cleaned up {deleted} records outside year range {min(keep_years)}-{max(keep_years)}\")\n",
    "\n",
    "for key in ['tb_incidence', 'tb_mortality']:\n",
    "    if key not in datasets:\n",
    "        continue\n",
    "    \n",
    "    df = datasets[key].copy()\n",
    "    df = df[df['year'].isin(YEARS)].copy()\n",
    "    \n",
    "    country_df = df[['country_code', 'region_code', 'region_name']].drop_duplicates()\n",
    "    country_df = country_df.dropna(subset=['country_code'])\n",
    "    \n",
    "    for _, row in country_df.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO country_metadata (country_code, region_code, region_name)\n",
    "            VALUES (:country_code, :region_code, :region_name)\n",
    "        \"\"\", {\n",
    "            'country_code': row['country_code'],\n",
    "            'region_code': row.get('region_code'),\n",
    "            'region_name': row.get('region_name')\n",
    "        })\n",
    "    \n",
    "    inserted = 0\n",
    "    column_name = 'tb_incidence_num' if key == 'tb_incidence' else 'tb_mortality_num'\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO tb_data \n",
    "            (country_code, year, {column_name})\n",
    "            VALUES (:country_code, :year, :value)\n",
    "            ON CONFLICT(country_code, year) DO UPDATE SET\n",
    "                {column_name} = excluded.{column_name}\n",
    "        \"\"\", {\n",
    "            'country_code': row['country_code'],\n",
    "            'year': int(row['year']),\n",
    "            'value': row.get('value')\n",
    "        })\n",
    "        inserted += 1\n",
    "    \n",
    "    print(f\"Loaded {inserted} TB {key} records (years {min(df['year'])}-{max(df['year'])})\")\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "try:\n",
    "    cursor.execute(\"DROP VIEW IF EXISTS unified_tb_data\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS unified_tb_data\")\n",
    "except sqlite3.OperationalError:\n",
    "    pass\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE unified_tb_data AS\n",
    "    SELECT \n",
    "        cm.country_code, cm.region_code, cm.region_name,\n",
    "        tb.year, \n",
    "        tb.tb_incidence_num AS number_of_cases,\n",
    "        tb.tb_mortality_num AS number_of_deaths\n",
    "    FROM country_metadata cm\n",
    "    INNER JOIN tb_data tb ON cm.country_code = tb.country_code\n",
    "    ORDER BY cm.country_code, tb.year\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"Data acquisition complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: Verify Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Coverage by Year: 885 total records\n",
      "    country_code region_code            region_name  year  number_of_cases  \\\n",
      "0            AFG         EMR  Eastern Mediterranean  2020             13.0   \n",
      "1            AGO         AFR                 Africa  2020          15000.0   \n",
      "2            ALB         EUR                 Europe  2020              4.0   \n",
      "3            ARE         EMR  Eastern Mediterranean  2020              5.0   \n",
      "4            ARG         AMR               Americas  2020            730.0   \n",
      "..           ...         ...                    ...   ...              ...   \n",
      "880          VUT         WPR        Western Pacific  2024              2.0   \n",
      "881          YEM         EMR  Eastern Mediterranean  2024            120.0   \n",
      "882          ZAF         AFR                 Africa  2024         134000.0   \n",
      "883          ZMB         AFR                 Africa  2024          19000.0   \n",
      "884          ZWE         AFR                 Africa  2024          20000.0   \n",
      "\n",
      "     number_of_deaths  \n",
      "0             14000.0  \n",
      "1             21000.0  \n",
      "2                 9.0  \n",
      "3                64.0  \n",
      "4               580.0  \n",
      "..                ...  \n",
      "880              27.0  \n",
      "881            2100.0  \n",
      "882           25000.0  \n",
      "883            4100.0  \n",
      "884            3000.0  \n",
      "\n",
      "[885 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(DATABASE_PATH, timeout=30.0)\n",
    "conn.row_factory = sqlite3.Row\n",
    "conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "\n",
    "summary = pd.read_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM unified_tb_data\n",
    "    ORDER BY year\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Data Coverage by Year: {len(summary)} total records\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
